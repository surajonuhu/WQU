{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<p>\n",
    "  <b>AI Lab: Deep Learning for Computer Vision</b><br>\n",
    "  <b><a href=\"https://www.wqu.edu/\">WorldQuant University</a></b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "  <p>\n",
    "    <center><b>Usage Guidelines</b></center>\n",
    "  </p>\n",
    "  <p>\n",
    "    This file is licensed under <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International</a>.\n",
    "  </p>\n",
    "  <p>\n",
    "    You <b>can</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Download this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Post this file in public repositories</li>\n",
    "    </ul>\n",
    "    You <b>must always</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: green\">‚úì</span> Give credit to <a href=\"https://www.wqu.edu/\">WorldQuant University</a> for the creation of this file</li>\n",
    "      <li><span style=\"color: green\">‚úì</span> Provide a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">link to the license</a></li>\n",
    "    </ul>\n",
    "    You <b>cannot</b>:\n",
    "    <ul>\n",
    "      <li><span style=\"color: red\">‚úó</span> Create derivatives or adaptations of this file</li>\n",
    "      <li><span style=\"color: red\">‚úó</span> Use this file for commercial purposes</li>\n",
    "    </ul>\n",
    "  </p>\n",
    "  <p>\n",
    "    Failure to follow these guidelines is a violation of your terms of service and could lead to your expulsion from WorldQuant University and the revocation your certificate.\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Before we can start this lesson, we need to import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import draw_bounding_boxes, make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Next, print the version numbers of the primary software to improve reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: linux\n",
      "Python version: 3.11.0 (main, Nov 15 2022, 20:12:54) [GCC 10.2.1 20210110]\n",
      "---\n",
      "CV2 version :  4.10.0\n",
      "torch version :  2.2.2+cu121\n",
      "torchvision version :  0.17.2+cu121\n"
     ]
    }
   ],
   "source": [
    "print(\"Platform:\", sys.platform)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"---\")\n",
    "print(\"CV2 version : \", cv2.__version__)\n",
    "print(\"torch version : \", torch.__version__)\n",
    "print(\"torchvision version : \", torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    " We are ready to start looking at the data. üèéÔ∏èüí®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Exploring Our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "In this project, we'll work with two datasets. Let's start with the [Dhaka AI dataset](https://www.kaggle.com/datasets/rifat963/dhakaai-dhaka-based-traffic-detection-dataset), which contains images of vehicles in urban traffic scenes from Dhaka, Bangladesh. This dataset is particularly interesting for computer vision as it captures the unique characteristics of Dhaka's busy streets, including a diverse mix of vehicle types and dense traffic conditions.\n",
    "\n",
    "We'll use the dataset for object detection which is a more complex task than image classification. Object detection identifies specific objects within an image (e.g., cars, buses, motorcycles), determines the precise location of these objects, and draws a bounding box around each detected object.\n",
    "\n",
    "This dataset will be used in a later lesson to create a custom model. For now, we'll begin by exploring the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ea780-3dab-4aa2-a2e3-bea76b9e38e1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color: #000\">\n",
    "    <p>\n",
    "  <strong>Warning: difference with video</strong>\n",
    "        </p>\n",
    "    <p>\n",
    "  The video associated to this lesson shows the instructor downloading Dhaka AI dataset from GCP. We've modified this project to make that data available without the need of downloading it. You can skip the parts where the instructor downloads and extracts the compressed data.\n",
    "    </p>\n",
    "    <p>The resulting directory and data are the same.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**Task 3.2.1:** Create a variable for the train directory using `pathlib` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: data_images/train\n"
     ]
    }
   ],
   "source": [
    "dhaka_image_dir = Path(\"data_images\", \"train\")\n",
    "\n",
    "print(\"Data directory:\", dhaka_image_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let's examine some of the contents of the train directory. You'll see two types of files:\n",
    "\n",
    "1. `.xml` files: These contain the annotations for the images.\n",
    "2. `.jpg` files: These are the actual image files.\n",
    "\n",
    "Each image typically has a corresponding XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data_images/train/Dipto_442.xml'),\n",
       " PosixPath('data_images/train/Numan_(56).jpg'),\n",
       " PosixPath('data_images/train/Navid_323.xml'),\n",
       " PosixPath('data_images/train/Navid_97.jpg'),\n",
       " PosixPath('data_images/train/Navid_181.jpg')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhaka_files = list(dhaka_image_dir.iterdir())\n",
    "dhaka_files[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Even though we only see one type of image file, it turns out that the image files can have many different possible extensions. Let's count the file extensions by type and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with extension .xml: 3003\n",
      "Files with extension .jpg: 2844\n",
      "Files with extension .JPG: 143\n",
      "Files with extension .png: 12\n",
      "Files with extension .jpeg: 2\n",
      "Files with extension .PNG: 2\n"
     ]
    }
   ],
   "source": [
    "file_extension_counts = Counter(Path(file).suffix for file in dhaka_files)\n",
    "\n",
    "for extension, count in file_extension_counts.items():\n",
    "    print(f\"Files with extension {extension}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Separating images and bounding boxes data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Bounding boxes are rectangles around a detected object. All bounding box information is contained in the `.xml` files. The images have several different extensions.   It makes sense to separate the different file types into different folders. We'll want to put all `.xml` files in an annotations folder and the various image types in an images folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "**Task 3.2.2:** Create variables for the images and annotations directories using `pathlib` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = dhaka_image_dir /\"images\"\n",
    "annotations_dir = dhaka_image_dir /\"annotations\"\n",
    "\n",
    "images_dir.mkdir(exist_ok=True)\n",
    "annotations_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "**Task 3.2.3:** Move files to the appropriate directory based on file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in dhaka_files:\n",
    "    if file.suffix.lower() in (\".jpg\", \".jpeg\", \".png\"):\n",
    "        target_dir = images_dir\n",
    "    elif file.suffix.lower() == \".xml\":\n",
    "        target_dir = annotations_dir\n",
    "    file.rename(target_dir / file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Let's confirm that all the files where moved by making sure there is equal number of images and annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_files = list(images_dir.iterdir())\n",
    "annotations_files = list(annotations_dir.iterdir())\n",
    "\n",
    "assert len(images_files) == len(annotations_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "The annotations are the labels for the data. Each image has an annotation that contains the coordinates and type of object for each bounding box in a given image.\n",
    "\n",
    "Let's look at the structure of the annotations by loading the first 25 lines of a file. The annotations are stored as XML which is a way to store structured documents. The `<annotation>` tag is the root element, containing all the information about this particular image annotation. The tags within store other information such as `<folder>`. The most important tag for the current project is the `<object>`. It describes an object detected in the image, this associated image contains a \"bus\". The tag `<bndbox>` is the bounding box information. There are the coordinates of a rectangle surrounding the bus in the image (in pixels): `<xmin>` is the left edge, `<ymin>` is the top edge, `<xmax>` is the right edge, and `<ymax>` is the bottom edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68e5f243-7fbd-45ca-9a26-a463aef51daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annotation>\n",
      "\t<folder>Images</folder>\n",
      "\t<filename>02_Motijheel_280714_0005.jpg</filename>\n",
      "\t<path>E:\\Datasets\\Dataset\\Images\\02_Motijheel_280714_0005.jpg</path>\n",
      "\t<source>\n",
      "\t\t<database>Unknown</database>\n",
      "\t</source>\n",
      "\t<size>\n",
      "\t\t<width>1200</width>\n",
      "\t\t<height>800</height>\n",
      "\t\t<depth>3</depth>\n",
      "\t</size>\n",
      "\t<segmented>0</segmented>\n",
      "\t<object>\n",
      "\t\t<name>bus</name>\n",
      "\t\t<pose>Unspecified</pose>\n",
      "\t\t<truncated>1</truncated>\n",
      "\t\t<difficult>0</difficult>\n",
      "\t\t<bndbox>\n",
      "\t\t\t<xmin>833</xmin>\n",
      "\t\t\t<ymin>390</ymin>\n",
      "\t\t\t<xmax>1087</xmax>\n",
      "\t\t\t<ymax>800</ymax>\n",
      "\t\t</bndbox>\n",
      "\t</object>\n"
     ]
    }
   ],
   "source": [
    "xml_filepath = annotations_dir / \"01.xml\"\n",
    "!head -n 25 $xml_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "The ElementTree (ET) module in Python can parse an XML file. In XML, the root is the top-level element that contains all other elements. The `tag` attribute contains the name of the element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse(xml_filepath)\n",
    "root = tree.getroot()\n",
    "print(root.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "The `find` method is used to locate the first occurrence of a sub-element with a given tag. Let's find the width and height of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image width: 1200  image height: 800\n"
     ]
    }
   ],
   "source": [
    "width = int(root.find(\"size\").find(\"width\").text)\n",
    "height = int(root.find(\"size\").find(\"height\").text)\n",
    "print(f\"image width: {width}  image height: {height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "The `findall` method finds all occurrences of a sub-element within a given tag. We can use that method to get all the relevant data for the bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "**Task 3.2.4:** Find the labels and coordinates for all the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bus: [833, 390, 1087, 800]\n",
      "bus: [901, 284, 1018, 395]\n",
      "bus: [909, 241, 1010, 287]\n",
      "rickshaw: [761, 413, 832, 540]\n",
      "rickshaw: [777, 364, 828, 409]\n",
      "rickshaw: [120, 351, 177, 423]\n",
      "rickshaw: [178, 340, 245, 419]\n",
      "rickshaw: [551, 229, 581, 267]\n",
      "rickshaw: [849, 211, 870, 240]\n",
      "rickshaw: [854, 191, 872, 208]\n",
      "rickshaw: [395, 250, 437, 286]\n",
      "rickshaw: [626, 209, 653, 240]\n",
      "motorbike: [863, 241, 882, 268]\n",
      "car: [218, 252, 289, 285]\n",
      "car: [495, 216, 531, 244]\n",
      "car: [485, 201, 520, 219]\n",
      "three wheelers (CNG): [254, 347, 298, 418]\n",
      "three wheelers (CNG): [398, 307, 457, 353]\n",
      "three wheelers (CNG): [240, 290, 303, 344]\n",
      "pickup: [933, 176, 959, 197]\n",
      "three wheelers (CNG): [709, 188, 728, 207]\n",
      "minivan: [575, 213, 600, 244]\n",
      "car: [530, 212, 549, 237]\n",
      "minivan: [592, 187, 633, 197]\n",
      "suv: [616, 198, 654, 212]\n",
      "suv: [879, 161, 918, 179]\n",
      "minivan: [850, 151, 882, 166]\n",
      "rickshaw: [844, 191, 856, 211]\n",
      "van: [827, 174, 852, 194]\n",
      "pickup: [410, 226, 452, 252]\n"
     ]
    }
   ],
   "source": [
    "bounding_boxes = []\n",
    "labels = []\n",
    "for obj in root.findall(\"object\"):\n",
    "    label = obj.find(\"name\").text\n",
    "    labels.append(label)\n",
    "    bndbox = obj.find(\"bndbox\")\n",
    "    xmin = int(bndbox.find(\"xmin\").text)\n",
    "    ymin = int(bndbox.find(\"ymin\").text)\n",
    "    xmax = int(bndbox.find(\"xmax\").text)\n",
    "    ymax = int(bndbox.find(\"ymax\").text)\n",
    "    bounding_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "for label, bounding_box in zip(labels, bounding_boxes):\n",
    "    print(f\"{label}: {bounding_box}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Bounding boxes in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "**Task 3.2.5:** Convert bounding boxes to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e80f8d5-8342-48a4-a1d3-01d4117d8835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 833.,  390., 1087.,  800.],\n",
      "        [ 901.,  284., 1018.,  395.],\n",
      "        [ 909.,  241., 1010.,  287.],\n",
      "        [ 761.,  413.,  832.,  540.],\n",
      "        [ 777.,  364.,  828.,  409.],\n",
      "        [ 120.,  351.,  177.,  423.],\n",
      "        [ 178.,  340.,  245.,  419.],\n",
      "        [ 551.,  229.,  581.,  267.],\n",
      "        [ 849.,  211.,  870.,  240.],\n",
      "        [ 854.,  191.,  872.,  208.],\n",
      "        [ 395.,  250.,  437.,  286.],\n",
      "        [ 626.,  209.,  653.,  240.],\n",
      "        [ 863.,  241.,  882.,  268.],\n",
      "        [ 218.,  252.,  289.,  285.],\n",
      "        [ 495.,  216.,  531.,  244.],\n",
      "        [ 485.,  201.,  520.,  219.],\n",
      "        [ 254.,  347.,  298.,  418.],\n",
      "        [ 398.,  307.,  457.,  353.],\n",
      "        [ 240.,  290.,  303.,  344.],\n",
      "        [ 933.,  176.,  959.,  197.],\n",
      "        [ 709.,  188.,  728.,  207.],\n",
      "        [ 575.,  213.,  600.,  244.],\n",
      "        [ 530.,  212.,  549.,  237.],\n",
      "        [ 592.,  187.,  633.,  197.],\n",
      "        [ 616.,  198.,  654.,  212.],\n",
      "        [ 879.,  161.,  918.,  179.],\n",
      "        [ 850.,  151.,  882.,  166.],\n",
      "        [ 844.,  191.,  856.,  211.],\n",
      "        [ 827.,  174.,  852.,  194.],\n",
      "        [ 410.,  226.,  452.,  252.]])\n"
     ]
    }
   ],
   "source": [
    "bboxes_tensor = torch.tensor(bounding_boxes, dtype=torch.float)\n",
    "print(bboxes_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "**Task 3.2.6:** Create a variable for the image path using `pathlib` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = ...\n",
    "image = read_image(str(image_path))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "We can use the `draw_bounding_boxes` function to add the bounding boxes and labels to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = draw_bounding_boxes(\n",
    "    image=image,\n",
    "    boxes=bboxes_tensor,\n",
    "    labels=labels,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font=\"arial.ttf\",\n",
    "    font_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "**Task 3.2.7:** Display the composite image with bounding boxes and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pil_image(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### YouTube Video Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Next, we load YouTube video of traffic in Dhaka, Bangladesh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "**Task 3.2.8:** Create a variable for the video directory using `pathlib` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_dir = Path(\"data_video\")\n",
    "video_name = \"dhaka_traffic.mp4\"\n",
    "video_path = ...\n",
    "\n",
    "print(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882559e8-e888-4c06-98b5-b56d6f3ea57f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" style=\"color: #000\">\n",
    "    <p>\n",
    "  <strong>Warning: difference with video</strong>\n",
    "        </p>\n",
    "    <p>\n",
    "  The video associated to this lesson shows the instructor downloading the YouTube video for this lesson directly from YouTube, or providing an alternative method downloading said video from GCP.\n",
    "    </p>\n",
    "    <p>We've modified this project to make that video available without the need of downloading it. You can skip the parts where the instructor downloads the video from YouTube or shows the alternative methods.</p>\n",
    "    <p>The resulting directory and video location are the same.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "The video is already available in the correct location. Let's look at the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(video_path, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "We are going to capture still images from the video to make it easier to draw bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not video_capture.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    frame_rate = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Frame rate: {frame_rate}\")\n",
    "    print(f\"Total number of frames: {frame_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Let's look the first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "success, first_frame = video_capture.read()\n",
    "if success:\n",
    "    plt.imshow(cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"First Frame\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Could not read frame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "We were successful in displaying the first frame, however that frame is black. Let's look at a later frame that is more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture.set(cv2.CAP_PROP_POS_FRAMES, 100)\n",
    "success, later_frame = video_capture.read()\n",
    "if success:\n",
    "    plt.imshow(cv2.cvtColor(later_frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"First Frame\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: Could not read frame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "**Task 3.2.9:** Create a directory for the frames using the `pathlib` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dir = ...\n",
    "frames_dir.mkdir(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "Now we walk through the video, saving selected frames as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    success, frame = video_capture.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Save frames at the frame_rate\n",
    "    if frame_count % frame_rate == 0:\n",
    "        frame_path = frames_dir / f\"frame_{frame_count}.jpg\"\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "We can look at the frames we have extracted and saved using the `display_sample_images` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(dir_path, sample=5):\n",
    "    image_list = []\n",
    "    images = sorted(dir_path.iterdir())\n",
    "    if images:\n",
    "        sample_images = images[:sample]\n",
    "        for sample_image in sample_images:\n",
    "            image = read_image(str(sample_image))\n",
    "            resize_transform = transforms.Resize((240, 240))\n",
    "            image = resize_transform(image)\n",
    "            image_list.append(image)\n",
    "    grid = make_grid(image_list, nrow=5)\n",
    "    image = to_pil_image(grid)\n",
    "    return image\n",
    "\n",
    "\n",
    "display_sample_images(frames_dir, sample=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "This file &#169; 2024 by [WorldQuant University](https://www.wqu.edu/) is licensed under [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
